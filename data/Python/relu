순전파 활성화 함수(relu)<br>
신경망에서 은닉층의 각 노드에 들어오는 값을 비선형 함수를 통과시킨 후 다음 계층으로 전달하는 함수입니다.<br>
0보다 작은 경우에는 0을 출력하고, 0보다 큰 경우는 항등함수를 사용합니다.<br>
아래는 relu함수와 출력 예시 입니다.<br>
<img src="relu.png" width="200">
<img src="reluPrint.png" width="200"><br>